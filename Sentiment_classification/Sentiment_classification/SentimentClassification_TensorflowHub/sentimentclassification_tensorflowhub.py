# -*- coding: utf-8 -*-
"""SentimentClassification_TensorflowHub.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IzILM4Cf6IImbPe6CdJBAsfOUwK9BOva

We would run this notebook in Colab and we would upload the Tweets.csv to content folder.Referred doc-https://www.tensorflow.org/tutorials/keras/text_classification_with_hub

Installing tensorflow hub and Importing required packages
"""

pip install tensorflow-hub

import keras
import pandas as pd
import tensorflow as tf
import tensorflow_hub as hub

from tensorflow.keras import layers, losses, Sequential, optimizers, metrics

hub.__version__

"""Loading the  data"""

columns = ["id", "country", "Label", "Text"]

tweets_data = pd.read_csv("twitter_training.csv", names = columns)

tweets_data.sample(5)

"""Dropping irrelevant columns, NAs and duplicates"""

tweets_data = tweets_data.drop(columns = ["id", "country"])

tweets_data.dropna(inplace = True, axis = 0 )

tweets_data = tweets_data.drop_duplicates()

tweets_data.shape

"""Converting the labels to numeric form"""

tweets_data["Label"] = tweets_data["Label"].replace({"Negative": 0, "Neutral": 1, "Positive": 2, "Irrelevant": 3})

tweets_data.sample(5)

"""Data is split into training , validation, and testing sets"""

from sklearn.model_selection import train_test_split

X_train, X_test = train_test_split(
    tweets_data, test_size = 0.2, stratify = tweets_data["Label"], random_state = 123)
X_train, X_val = train_test_split(
    X_train, test_size = 0.1, stratify = X_train["Label"], random_state = 123)

X_train.shape, X_val.shape, X_test.shape

"""Creating Training and validation dataset from corresponding pandas dataframes"""

BATCH_SIZE = 128

raw_train_ds = tf.data.Dataset.from_tensor_slices(
    (X_train["Text"].values, X_train["Label"].values)).shuffle(10000).batch(batch_size = BATCH_SIZE)

raw_val_ds = tf.data.Dataset.from_tensor_slices(
    (X_val["Text"].values, X_val["Label"].values)).batch(batch_size = BATCH_SIZE)

raw_test_ds = tf.data.Dataset.from_tensor_slices(
    (X_test["Text"].values, X_test["Label"].values)).batch(batch_size = BATCH_SIZE)

train_examples_batch, train_labels_batch = next(iter(raw_train_ds))

train_examples_batch[:5]

train_labels_batch[:5]

"""Model Building
The neural network is created by stacking layers—this requires three main architectural decisions:

- How to represent the text?
- How many layers to use in the model?
- How many hidden units to use for each layer?
- In this example, the input data consists of sentences. The labels to predict are either 0, 1, 2, 3.

One way to represent the text is to convert sentences into embeddings vectors. Use a pre-trained text embedding as the first layer, which will have three advantages:

- You don't have to worry about text preprocessing,
- Benefit from transfer learning,
- the embedding has a fixed size, so it's simpler to process.

For this example you use a pre-trained text embedding model from TensorFlow Hub called google/nnlm-en-dim50/2.

There are many other pre-trained text embeddings from TFHub that can be used in this tutorial:

- google/nnlm-en-dim128/2 - trained with the same NNLM architecture on the same - data as google/nnlm-en-dim50/2, but with a larger embedding dimension. Larger dimensional embeddings can improve on your task but it may take longer to train your model.
- google/nnlm-en-dim128-with-normalization/2 - the same as google/nnlm-en-dim128/2, but with additional text normalization such as removing punctuation. This can help if the text in your task contains additional characters or punctuation.
- google/universal-sentence-encoder/4 - a much larger model yielding 512 dimensional embeddings trained with a deep averaging network (DAN) encoder.


Creating a Keras layer that uses a TensorFlow Hub model to embed the sentences.Note that no matter the length of the input text, the output shape of the embeddings is: (num_examples, embedding_dimension).Best test accuracy of around 0.87  is obtained with 'google/nnlm-en-dim128-with-normalization/2' and with other two embeddings i.e google/nnlm-en-dim50/2 and google/nnlm-en-dim128/2 test accuracies obtained is around 0.0.82

### TODO Recording:

- When you record, please show only one embedding at a time (DO NOT show commented out text)
"""

# embedding = "https://tfhub.dev/google/nnlm-en-dim50/2"

# embedding = "https://tfhub.dev/google/nnlm-en-dim128/2"
embedding = "https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2"

hub_layer = hub.KerasLayer(
    embedding, input_shape = [],
    dtype = tf.string, trainable = True
)

hub_layer(train_examples_batch[:3]).shape

"""Building the full model"""

model = Sequential()

model.add(hub_layer)
model.add(layers.Dense(32, activation = "relu"))
model.add(layers.Dense(4))

model.summary()

"""Model is compiled with Adam optimizer and loss function-Sparse Categorical Cross Entropy"""

model.compile(
    optimizer = keras.optimizers.Adam(learning_rate = 0.001),
    loss = losses.SparseCategoricalCrossentropy(from_logits = True),
    metrics = ['accuracy']
)

"""We are using a callback list here
Early stopping — Interrupting training when the validation loss is no longer improving (and save the best model obtained during training).

ReduceLROnPlateau-Dynamically adjusting the value of certain parameters during training such as the learning rate optimizer.

Callbacks are passed to the during via the callback argument in the fit() method which takes a list of callbacks. Any number of callbacks can be passed to it.

The monitor argument in the EarlyStopping callback monitor’s the model’s validation accuracy and the patience argument interrupts training when the parameter passed to the monitor argument stops improving for more than the number (of epochs) passed to it (in this case 1).

Also, the ReduceLROnPlateau callback is used to reduce the learning rate when the validation loss has stopped improving. This has proven to be a very effective strategy to get out of local minima during training. The factor argument takes as input a float which is used to divide the learning rate when triggered.
"""

callback_list = [
    keras.callbacks.EarlyStopping(
        patience = 3,
        monitor = "val_accuracy"
    ),

    keras.callbacks.ReduceLROnPlateau(
        patience = 1,
        factor = 0.1,
    )
]

"""Model is trained for 20 epochs"""

EPOCHS = 20

history = model.fit(
    raw_train_ds,
    validation_data = raw_val_ds,
    epochs = EPOCHS,
    callbacks = callback_list
)

"""Test accuracy is obtained"""

loss, accuracy = model.evaluate(raw_test_ds)

print("Loss: ", loss)
print("Accuracy: ", accuracy)


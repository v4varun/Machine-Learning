# -*- coding: utf-8 -*-
"""SentimentClassification_FNet_.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KCjwGaMvGv0Z6f_8M3i5H2NgCwgOENFD

Referred documentation notebook-
https://keras.io/examples/nlp/fnet_classification_with_keras_nlp/
"""

!pip install tensorflow-text

!pip install keras-nlp

import keras
import keras_nlp

import pandas as pd
import tensorflow as tf

from tensorflow.keras import layers, losses, optimizers

"""Loading the data"""

columns = ["id", "country", "Label", "Text"]

tweets_data = pd.read_csv("twitter_training.csv", names = columns)

tweets_data.sample(5)

"""Dropping irrelevant columns,rows with NAs and  duplicates"""

tweets_data = tweets_data.drop(columns = ["id", "country"])

tweets_data.dropna(inplace = True, axis = 0)

tweets_data = tweets_data.drop_duplicates()

tweets_data.shape

"""Converting text labels to numeric form"""

tweets_data["Label"] = tweets_data["Label"].replace({"Negative": 0, "Neutral": 1, "Positive": 2, "Irrelevant": 3})

tweets_data.sample(5)

from sklearn.model_selection import train_test_split

X_train, X_test = train_test_split(
    tweets_data, test_size = 0.2, stratify = tweets_data["Label"], random_state = 123)
X_train, X_val = train_test_split(
    X_train, test_size = 0.1, stratify = X_train["Label"], random_state = 123)

X_train.shape, X_val.shape, X_test.shape

"""Creating Training and validation dataset from corresponding pandas dataframes"""

BATCH_SIZE = 128

train_ds = tf.data.Dataset.from_tensor_slices(
    (X_train["Text"].values, X_train["Label"].values)).shuffle(10000).batch(batch_size = BATCH_SIZE)

val_ds = tf.data.Dataset.from_tensor_slices(
    (X_val["Text"].values, X_val["Label"].values)).batch(batch_size = BATCH_SIZE)

test_ds = tf.data.Dataset.from_tensor_slices(
    (X_test["Text"].values, X_test["Label"].values)).batch(batch_size = BATCH_SIZE)

train_ds = train_ds.map(lambda x, y: (tf.strings.lower(x), y))

val_ds = val_ds.map(lambda x, y: (tf.strings.lower(x), y))

test_ds = test_ds.map(lambda x, y: (tf.strings.lower(x), y))

for text_batch, label_batch in train_ds.take(1):
    for i in range(3):
        print(text_batch.numpy()[i])
        print(label_batch.numpy()[i])

"""#### Tokenizing the data

WordPiece tokenization

Purpose: The primary purpose of WordPiece is to split text into a set of common subword units or tokens. This approach helps in handling the large vocabulary issue in language models and improves the model's ability to deal with rare words or out-of-vocabulary (OOV) words.

How It Works: WordPiece starts with a base vocabulary of individual characters and then incrementally learns a larger vocabulary by combining these characters into frequently occurring substrings or subwords. The algorithm iteratively adds the best subword (the one that minimizes the language model's loss function) to the vocabulary until it reaches a specified vocabulary size.

Subword Tokenization: The resulting vocabulary consists of full words, subwords, and characters. Full words are common words that appear frequently in the training corpus. Subwords are parts of words that are less common but still occur frequently enough to be included. Characters are included to ensure any word can be tokenized (e.g., rare words are broken down into individual characters).


We'll be using the keras_nlp.tokenizers.WordPieceTokenizer layer to tokenize the text. keras_nlp.tokenizers.WordPieceTokenizer takes a WordPiece vocabulary and has functions for tokenizing the text, and detokenizing sequences of tokens.

Before we define the tokenizer, we first need to train it on the dataset we have. The WordPiece tokenization algorithm is a subword tokenization algorithm; training it on a corpus gives us a vocabulary of subwords. A subword tokenizer is a compromise between word tokenizers (word tokenizers need very large vocabularies for good coverage of input words), and character tokenizers (characters don't really encode meaning like words do). Luckily, KerasNLP makes it very simple to train WordPiece on a corpus with the keras_nlp.tokenizers.compute_word_piece_vocabulary utility.

Note: The official implementation of FNet uses the SentencePiece Tokenizer.
"""

def train_word_piece(ds, vocab_size, reserved_tokens):

    word_piece_ds = ds.unbatch().map(lambda x, y: x)

    vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(
        word_piece_ds.batch(1000).prefetch(2),
        vocabulary_size = vocab_size,
        reserved_tokens = reserved_tokens,
    )

    return vocab

"""Every vocabulary has a few special, reserved tokens. We have two such tokens:

- "[PAD]" - Padding token. Padding tokens are appended to the input sequence length when the input sequence length is shorter than the maximum sequence length.
- "[UNK]" - Unknown token.
"""

vocab_size = 10000

reserved_tokens = ["[PAD]", "[UNK]"]

# train_sentences = [element[0] for element in train_ds]
vocab = train_word_piece(train_ds, vocab_size, reserved_tokens)

"""Length of vocabulary is checked and also whole vocab is viewed"""

len(vocab)

vocab

"""Now, let's define the tokenizer. We will configure the tokenizer with the the vocabularies trained above. We will define a maximum sequence length so that all sequences are padded to the same length, if the length of the sequence is less than the specified sequence length. Otherwise, the sequence is truncated."""

max_sequence_length = 64

tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(
    vocabulary = vocab,
    lowercase = False,
    sequence_length = max_sequence_length,
)

"""Let's try and tokenize a sample from our dataset! To verify whether the text has been tokenized correctly, we can also detokenize the list of tokens back to the original text.


"""

input_sentence_ex = train_ds.take(1).get_single_element()[0][1]
input_tokens_ex = tokenizer(input_sentence_ex)

print("Sentence: ", input_sentence_ex)
print("Tokens: ", input_tokens_ex)
print("Recovered text after detokenizing: ", tokenizer.detokenize(input_tokens_ex))

"""Next, we'll format our datasets in the form that will be fed to the models. We need to tokenize the text."""

def format_dataset(sentence, label):

    sentence = tokenizer(sentence)

    return ({"input_ids": sentence}, label)


def make_dataset(dataset):

    dataset = dataset.map(format_dataset, num_parallel_calls = tf.data.AUTOTUNE)

    return dataset.shuffle(10000).prefetch(512).cache()


train_ds = make_dataset(train_ds)
val_ds = make_dataset(val_ds)
test_ds = make_dataset(test_ds)

epochs = 5

embedding_dim = 128
intermediate_dim = 256

"""Now, let's move on to the exciting part - defining our model! We first need an embedding layer, i.e., a layer that maps every token in the input sequence to a vector. This embedding layer can be initialised randomly. We also need a positional embedding layer which encodes the word order in the sequence. The convention is to add, i.e., sum, these two embeddings. KerasNLP has a keras_nlp.layers.TokenAndPositionEmbedding layer which does all of the above steps for us.

Our FNet classification model consists of three keras_nlp.layers.FNetEncoder layers with a keras.layers.Dense layer on top.

Note: For FNet, masking the padding tokens has a minimal effect on results. In the official implementation, the padding tokens are not masked.
"""

input_ids = keras.Input(shape = (None,), dtype = "int64", name = "input_ids")

x = keras_nlp.layers.TokenAndPositionEmbedding(
    vocabulary_size = vocab_size,
    sequence_length = max_sequence_length,
    embedding_dim = embedding_dim,
    mask_zero = True,
)(input_ids)

x = keras_nlp.layers.FNetEncoder(intermediate_dim = intermediate_dim)(inputs = x)
x = keras_nlp.layers.FNetEncoder(intermediate_dim = intermediate_dim)(inputs = x)
x = keras_nlp.layers.FNetEncoder(intermediate_dim = intermediate_dim)(inputs = x)

x = keras.layers.GlobalAveragePooling1D()(x)
x = keras.layers.Dropout(0.1)(x)

outputs = keras.layers.Dense(4, activation = "softmax")(x)

fnet_classifier = keras.Model(input_ids, outputs, name = "fnet_classifier")

"""We'll use accuracy to monitor training progress on the validation data. Let's train our model for 5 epochs."""

fnet_classifier.summary()

fnet_classifier.compile(
    optimizer = optimizers.Adam(learning_rate = 0.001),
    loss = losses.SparseCategoricalCrossentropy(from_logits = False),
    metrics = ["accuracy"],
)

fnet_classifier.fit(train_ds, epochs = epochs, validation_data = val_ds)

"""Performance is checked on Test data"""

loss, accuracy = fnet_classifier.evaluate(test_ds)

print("Loss: ", loss)
print("Accuracy: ", accuracy)


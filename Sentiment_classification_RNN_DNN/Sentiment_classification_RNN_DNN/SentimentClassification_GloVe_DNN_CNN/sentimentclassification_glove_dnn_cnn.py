# -*- coding: utf-8 -*-
"""SentimentClassification_GloVe_DNN_CNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NJMVopSBDMMoRgumEuvljmejZefrrrV4

### TODO Recording:

- Show that you are using the GPU runtime
- Open up the folder icon on the left
- Upload the twitter_training.csv file to this folder
"""

import re
import nltk
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import keras
import tensorflow as tf

from tensorflow.keras import layers, models, losses, Sequential, optimizers, metrics
from keras.layers import Conv1D, Dense, Embedding, Dropout, GlobalMaxPooling1D, SpatialDropout1D

"""Loading the  data. link-https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis?select=twitter_training.csv"""

columns = ["id", "country", "Label", "Text"]

tweets_data = pd.read_csv("twitter_training.csv", names = columns)

tweets_data.sample(10)

tweets_data.info()

"""Dropping irrelevant columns"""

tweets_data = tweets_data.drop(columns = ["id", "country"])

tweets_data.sample(10)

tweets_data.isna().sum()

"""Droppin rows with NAs in Text column"""

tweets_data.dropna(inplace = True, axis = 0)

tweets_data.isna().sum()

tweets_data.duplicated().sum()

"""Dropping duplicates"""

tweets_data = tweets_data.drop_duplicates()

tweets_data.shape

"""Imbalance is checked"""

tweets_data["Label"].value_counts()

"""Downloading stopwords and viewing all the stop words

Checking word counts for each instance of tweets
"""

tweets_data["word_count"] = tweets_data["Text"].apply(lambda x: len(str(x).split(" ")))

tweets_data.sample(5)

"""Mean count - 20 words
Median count - 16 words
Max count - 198 words
"""

tweets_data.describe()

"""Positive label has least mean word count and Neutral category has the highest mean word count"""

tweets_data.groupby("Label")["word_count"].mean()

tweets_data.groupby("Label")["word_count"].median()

from nltk.corpus import stopwords
nltk.download("stopwords")

stop_words = stopwords.words("english")

"""Custom text cleaning function is defined"""

import string

stopwords = r"\b(?:{})\b".format("|".join(stop_words))

def clean_and_standardize_text(input_data):

    # Convert to lowercase
    lowercase = tf.strings.lower(input_data)

    # Remove URLs
    stripped_urls = tf.strings.regex_replace(lowercase, r"https?://\S+|www\.\S+", "")

    # Remove email addresses and symbols
    stripped_symbol = tf.strings.regex_replace(stripped_urls, "\S*@\S*\s?", "")

    # Remove text in angular brackets (usually HTML tags)
    stripped_brackets = tf.strings.regex_replace(stripped_symbol, "<.*?>+", "")

    # Remove any square brackets and leave the text within square brackets
    stripped_brackets = tf.strings.regex_replace(stripped_brackets, "\[|\]", "")

    # Matches alphanumeric characters with digits and remove those
    stripped_digits = tf.strings.regex_replace(stripped_brackets, "\w*\d\w*", "")

    # Remove stopwords
    stripped_stopwords = tf.strings.regex_replace(stripped_digits, stopwords, "")

    # Replace multiple whitespaces with a single whitespace
    stripped_whitespace_chars = tf.strings.regex_replace(stripped_stopwords, "\s+", " ")

    # Remove non-alphabet characters
    return tf.strings.regex_replace(stripped_whitespace_chars ,r"[^a-zA-Z\s]+" ,"")

"""Converting text labels to numeric form"""

tweets_data["Label"] = tweets_data["Label"].replace({"Negative": 0, "Neutral": 1, "Positive": 2, "Irrelevant":3})

tweets_data.sample(10)

from sklearn.model_selection import train_test_split

X_train, X_test = train_test_split(tweets_data, test_size = 0.2,
                                   stratify = tweets_data["Label"], random_state = 123)
X_train, X_val = train_test_split(X_train, test_size = 0.1,
                                  stratify = X_train["Label"], random_state = 123)

X_train.shape, X_val.shape, X_test.shape

"""Creating Training and validation dataset from corresponding pandas dataframes"""

raw_train_ds = tf.data.Dataset.from_tensor_slices(
    (X_train["Text"].values, X_train["Label"].values)).shuffle(10000).batch(batch_size = 128)

raw_val_ds = tf.data.Dataset.from_tensor_slices(
    (X_val["Text"].values, X_val["Label"].values)).batch(batch_size = 128)

raw_test_ds = tf.data.Dataset.from_tensor_slices(
    (X_test["Text"].values, X_test["Label"].values)).batch(batch_size = 128)

"""Checking one batch of raw train data"""

text_batch, label_batch = next(iter(raw_train_ds))

text_batch[:5], label_batch[:5]

"""Next, we will create a TextVectorization layer. You will use this layer to standardize, tokenize, and vectorize our data. You set the output_mode to int to create unique integer indices for each token.

Note that you're using the default split function, and the custom standardization function we defined above. We also define some constants for the model, like an explicit maximum sequence_length, which will cause the layer to pad or truncate sequences to exactly sequence_length values.
"""

VOCAB_SIZE = 10000

SEQUENCE_LENGTH = 200

vectorize_layer = tf.keras.layers.TextVectorization(
    standardize = clean_and_standardize_text,
    max_tokens = VOCAB_SIZE,
    output_mode = "int",
    output_sequence_length = SEQUENCE_LENGTH
)

"""Next, we will call adapt to fit the state of the preprocessing layer to the dataset. This will cause the model to build an index of strings to integers."""

train_text = raw_train_ds.map(lambda x, y: x)

vectorize_layer.adapt(train_text)

"""Creating a function to see the result of using this layer to preprocess some data."""

def vectorize_text(text, label):

    text = tf.expand_dims(text, -1)

    return vectorize_layer(text), label

train_ds = raw_train_ds.map(vectorize_text)

val_ds = raw_val_ds.map(vectorize_text)

test_ds = raw_test_ds.map(vectorize_text)

AUTOTUNE = tf.data.AUTOTUNE

train_ds = train_ds.cache().prefetch(buffer_size = AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size = AUTOTUNE)
test_ds = test_ds.cache().prefetch(buffer_size = AUTOTUNE)

vocab = vectorize_layer.get_vocabulary()

vocab

vocab_size = len(vocab)

vocab_size

"""We are creating our embedding layer with vocab size and dimension specified

### GlobalMaxPooling1D

Purpose: The primary purpose of the GlobalMaxPooling1D layer is to reduce the dimensionality of the input. It does this by taking the maximum value over the time dimension. This operation helps in reducing the computational cost for the network and also helps in reducing overfitting by providing an abstracted form of the representation.

How it Works: Given an input tensor (for example, the output of a convolutional layer), this layer will calculate the maximum value for each feature map. If the input tensor has dimensions (batch_size, steps, features), the GlobalMaxPooling1D layer will output a tensor of shape (batch_size, features). It effectively takes the maximum value across the steps dimension for each feature.
"""

EMBEDDING_DIM = 64

model = Sequential()

model.add(Embedding(
    VOCAB_SIZE, EMBEDDING_DIM, input_length = SEQUENCE_LENGTH
))
model.add(GlobalMaxPooling1D())

model.add(Dense(32, activation = "tanh"))

model.add(Dense(4))

model.summary()

"""Model is compiled and Trained for 10 epochs and test accuracy of 0.85 is obtained"""

model.compile(
    optimizer = optimizers.Adam(learning_rate = 0.001),
    loss = losses.SparseCategoricalCrossentropy(from_logits = True),
    metrics = ["accuracy"]
)

EPOCHS = 10

history = model.fit(
    train_ds,
    validation_data = val_ds,
    epochs = EPOCHS
)

loss, accuracy = model.evaluate(test_ds)

print("Loss: ", loss)
print("Accuracy: ", accuracy)

"""Now we will be using pretrained embeddings so word index is created

Finding dict mapping words to their indices
"""

voc = vectorize_layer.get_vocabulary()

word_index = dict(zip(voc, range(len(voc))))

word_index

"""We are mounting the drive as Glove embeddings are loaded in Drive"""

from google.colab import drive

drive.mount("/content/drive/")

"""### TODO Recording:

- Open up the file in Sublimetext on local machine and show the structure

- go to https://drive.google.com
- show that there is a glove_embeddings/ folder
- upload the 100d embeddings to that folder behind the scenes
- show that it exists there

- Once we have mounted Google Drive, open up the folder icon on the left and show that drive/ is available there

Loading  pre-trained GloVe embeddings.
The archive contains text-encoded vectors of various sizes: 50-dimensional, 100-dimensional, 200-dimensional, 300-dimensional. We'll use the 100D ones.
Let's make a dict mapping words (strings) to their NumPy vector representation:

### np.fromstring(coefs, "f", sep = " ")

Input String (coefs): This is the string from which the NumPy array will be created. It is expected to contain numbers separated by spaces (or another separator, as specified).

Data Type ("f"): The "f" argument specifies the data type of the array to be created. In this case, "f" stands for float32, which is a 32-bit floating-point number. Each element in the resulting array will be of this type.

Separator (sep=" "): The sep=" " argument tells the function that the numbers in the input string are separated by spaces. This is how the function knows where one number ends and the next begins. If the numbers in your string are separated by commas, for example, you would use sep=",".
"""

import os

path_to_glove_file = '/content/drive/MyDrive/glove_embeddings/glove.6B.100d.txt'

embeddings_index = {}

with open(path_to_glove_file) as f:
    for line in f:
        word, coefs = line.split(maxsplit = 1)
        coefs = np.fromstring(coefs, "f", sep = " ")
        embeddings_index[word] = coefs

print("Found %s word vectors." % len(embeddings_index))

"""Now, Preparing a  corresponding embedding matrix that we can use in a Keras Embedding layer. It's a simple NumPy matrix where entry at index i is the pre-trained vector for the word of index i in our vectorizer's vocabulary."""

num_tokens = len(voc) + 2
embedding_dim = 100

hits = 0
misses = 0

# Prepare embedding matrix
embedding_matrix = np.zeros((num_tokens, embedding_dim))

for word, i in word_index.items():

    embedding_vector = embeddings_index.get(word)

    if embedding_vector is not None:
        # Words not found in embedding index will be all-zeros.
        # This includes the representation for "padding" and "OOV" (out-of-vocabulary)
        embedding_matrix[i] = embedding_vector
        hits += 1
    else:
        misses += 1
print("Converted %d words (%d misses)" % (hits, misses))

"""Next, we load the pre-trained word embeddings matrix into an Embedding layer.

### TODO Recording:

- We need to come back here and set trainable = True (will add a note below)
- We will also need to come back here and change to use Conv1D layer

First we set  trainable=False so as to keep the embeddings fixed (we don't want to update them during training).
"""

model = Sequential()

model.add(Embedding(
    num_tokens, embedding_dim,
    embeddings_initializer = tf.keras.initializers.Constant(embedding_matrix),
    trainable = True
))

model.add(GlobalMaxPooling1D())

model.add(Dense(32, activation = "tanh"))

model.add(Dense(4))

model.summary()

"""So, Using  pretrained embeddings , performance did not improve much in comparison to normal embeddings"""

model.compile(
    optimizer = optimizers.Adam(learning_rate = 0.001),
    loss = losses.SparseCategoricalCrossentropy(from_logits = True),
    metrics = ["accuracy"]
)

EPOCHS = 10

history = model.fit(
    train_ds,
    validation_data = val_ds,
    epochs = EPOCHS
)

"""when trainable=False, Accuracy is not good but when trainable=True , Accuracy jumps to 0.83"""

loss, accuracy = model.evaluate(test_ds)

print("Loss: ", loss)
print("Accuracy: ", accuracy)

"""### TODO Recording:

- After trainable=False go back to the previous cell and set trainable=True
- After trainable=True go back and change the model to use the CNN

Embedding visualization section

### TODO Recording:

- We can start with this link to visualise Word2Vec 10K embeddings-
https://projector.tensorflow.org/
- First zoom into the words and hover over a few words and show what they are
- Then zoom out and select a few words and see which words are closest in space
- Do this for 5-6 words
- Then come back to the notebook

Then we can visualise our pretrained embeddings
"""

from tensorboard.plugins import projector

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

"""TensorBoard reads tensors and metadata from the logs of your tensorflow projects. The path to the log directory is specified with log_dir below.

In order to load the data into Tensorboard, we need to save a training checkpoint to that directory, along with metadata that allows for visualization of a specific layer of interest in the model.
"""

# create logging directory
import os

log_dir = 'logs/fit/'
if not os.path.exists(log_dir):
    os.makedirs(log_dir)

# write wordindex dictionary
with open(os.path.join(log_dir, "metadata.tsv"), "w") as f:
    for w in voc:
        f.write("{}\n".format(w))

# weights from the embedding layer, in our case: model.layers[1]
weights = tf.Variable(model.layers[0].get_weights()[0][1:])

checkpoint = tf.train.Checkpoint(embedding = weights)
checkpoint.save(os.path.join(log_dir, "embedding.ckpt"))

# configuration set-up
config = projector.ProjectorConfig()
embedding = config.embeddings.add()
embedding.tensor_name = "embedding/.ATTRIBUTES/VARIABLE_VALUE"
embedding.metadata_path = "metadata.tsv"
projector.visualize_embeddings(log_dir, config)

"""### TODO Recording:

- First open up the folder on the left
- Show there is a logs/fit/ folder and show the sub-folders within it

### TODO Recording:

- In Tensorboard zoom in and hover over some points and show
- Zoom out and select 4-5 points and show nearest neighbours
- On the bottom left, uncheck the 3rd PCA component, show that now the words are in 2 dimensional space
- On the left change dimensionality reduction algorithm to TSNE (this will start running for some iterations)
- Let it run for about 25 iterations - stop the algorithm
- Now select 4-5 words and show
"""

# Commented out IPython magic to ensure Python compatibility.
# Now run tensorboard against on log data we just saved.
# %tensorboard --logdir=logs/fit/ --port=6007

int_sequences_input = tf.keras.Input(shape = (None, ), dtype = "int64")

embedded_sequences = Embedding(
    num_tokens, embedding_dim,
    input_length=SEQUENCE_LENGTH,
    embeddings_initializer = tf.keras.initializers.Constant(embedding_matrix),
    trainable = True
) (int_sequences_input)

x = Conv1D(filters = 128, kernel_size = 5, activation = "tanh")(embedded_sequences)

x = GlobalMaxPooling1D()(x)

x = Dense(64, activation = "tanh")(x)

x = Dropout(0.5)(x)

preds = Dense(4)(x)

model = tf.keras.Model(int_sequences_input, preds)

model.summary()

model.compile(
    optimizer = optimizers.Adam(learning_rate = 0.001),
    loss = losses.SparseCategoricalCrossentropy(from_logits = True),
    metrics = ["accuracy"]
)

EPOCHS = 10

history = model.fit(
    train_ds,
    validation_data = val_ds,
    epochs = EPOCHS
)

loss, accuracy = model.evaluate(test_ds)

print("Loss: ", loss)
print("Accuracy: ", accuracy)


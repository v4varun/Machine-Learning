{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["Keras Documentation reference-\n"," https://keras.io/examples/nlp/neural_machine_translation_with_transformer/\n"," Note:Keras 3.0 is used here\n","Required Libraries and modules are imported"],"metadata":{"id":"8kXozpxvYxfg"}},{"cell_type":"code","source":["import keras\n","\n","keras.__version__"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"rkkvcqh-aWuP","outputId":"ff0046d8-deeb-44ff-f676-8481a920bd71"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'2.15.0'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":1}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"trS5DKfgrjLE"},"outputs":[],"source":["import re\n","import string\n","import random\n","import pathlib\n","\n","import numpy as np\n","import tensorflow as tf\n","\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers import TextVectorization"]},{"cell_type":"markdown","source":["Loading the data and then generating sentence pairs in tuple form."],"metadata":{"id":"HE8oyoF0ZOj4"}},{"cell_type":"code","source":["with open('fra.txt') as f:\n","    lines = f.read().split(\"\\n\")[:-1]\n","\n","text_pairs = []\n","\n","for line in lines:\n","    eng, fra = line.split(\"\\t\")\n","    fra = \"[start] \" + fra + \" [end]\"\n","\n","    text_pairs.append((eng, fra))"],"metadata":{"id":"JEoCZJf_tjpB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Viewing some eng-french sentence pairs"],"metadata":{"id":"P-J69MA4a3gC"}},{"cell_type":"code","source":["for _ in range(10):\n","    print(random.choice(text_pairs))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fqTF8PQFtjjI","outputId":"a64dda66-44a3-41d9-cec6-8d3d0072f08a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["('This is preposterous.', \"[start] C'est grotesque. [end]\")\n","('I want to talk to all of them.', '[start] Je veux leur parler à toutes. [end]')\n","(\"You aren't as short as I am.\", \"[start] Vous n'êtes pas aussi petite que moi. [end]\")\n","(\"He can't be trusted.\", '[start] On ne peut pas se fier à lui. [end]')\n","('You were kind to help me.', \"[start] C'était gentil à vous de m'aider. [end]\")\n","('It was a national scandal.', '[start] Ce fut un scandale national. [end]')\n","('I want to please you.', '[start] Je veux te plaire. [end]')\n","(\"That's a myth.\", \"[start] C'est un mythe. [end]\")\n","('He is my best friend.', \"[start] C'est mon meilleur ami. [end]\")\n","(\"I've been sent to relieve you.\", \"[start] J'ai été envoyé pour vous soulager. [end]\")\n"]}]},{"cell_type":"markdown","source":["Splitting whole text pairs list into training, validation and test pairs"],"metadata":{"id":"cNMHp2xGbvSh"}},{"cell_type":"code","source":["random.shuffle(text_pairs)\n","\n","num_val_samples = int(0.15 * len(text_pairs))\n","\n","num_train_samples = len(text_pairs) - 2 * num_val_samples\n","\n","train_pairs = text_pairs[:num_train_samples]\n","val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n","test_pairs = text_pairs[num_train_samples + num_val_samples :]\n","\n","print(f\"{len(text_pairs)} total pairs\")\n","print(f\"{len(train_pairs)} training pairs\")\n","print(f\"{len(val_pairs)} validation pairs\")\n","print(f\"{len(test_pairs)} test pairs\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p83BMnfktjgg","outputId":"766f9cf6-4551-4e19-dd80-5f936f8c13e9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["167130 total pairs\n","116992 training pairs\n","25069 validation pairs\n","25069 test pairs\n"]}]},{"cell_type":"markdown","source":["Vectorizing the text data\n","Usinf two instances of the TextVectorization layer to vectorize the text data (one for English and one for French), that is to say, to turn the original strings into integer sequences where each integer represents the index of a word in a vocabulary.\n","\n","The English layer will use the default string standardization (strip punctuation characters) and splitting scheme (split on whitespace), while the French layer will use a custom standardization,\n","\n"],"metadata":{"id":"F9-xyRYDegM8"}},{"cell_type":"code","source":["strip_chars = string.punctuation\n","strip_chars = strip_chars.replace(\"[\", \"\")\n","strip_chars = strip_chars.replace(\"]\", \"\")\n","\n","vocab_size = 15000\n","sequence_length = 20\n","batch_size = 64\n","\n","def custom_standardization(input_string):\n","    lowercase = tf.strings.lower(input_string)\n","\n","    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n","\n","\n","eng_vectorization = TextVectorization(\n","    max_tokens = vocab_size,\n","    output_mode = \"int\",\n","    output_sequence_length = sequence_length,\n",")\n","\n","fra_vectorization = TextVectorization(\n","    max_tokens = vocab_size,\n","    output_mode = \"int\",\n","    output_sequence_length = sequence_length + 1,\n","    standardize = custom_standardization,\n",")\n"],"metadata":{"id":"EtS1-z5stjd3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Creating both eng and french vocabularies using training data"],"metadata":{"id":"WEq7tlpWfM2I"}},{"cell_type":"code","source":["train_eng_texts = [pair[0] for pair in train_pairs]\n","train_fra_texts = [pair[1] for pair in train_pairs]\n","\n","eng_vectorization.adapt(train_eng_texts)\n","fra_vectorization.adapt(train_fra_texts)"],"metadata":{"id":"TJwFmEZaeIlc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Checking the english vocabulary"],"metadata":{"id":"oq8U3GO7giiX"}},{"cell_type":"code","source":["vocab_inp_size = len(eng_vectorization.get_vocabulary())\n","\n","vocab_inp_size"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UnD42UwtgXeA","outputId":"f3f2c42c-8e7a-4e28-89bc-efaaf0d4ee7d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["12842"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["eng_vectorization.get_vocabulary()[:15]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LssRTVy9f7an","outputId":"377930c0-cb92-4dc7-fccd-72ca906c6ff5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['',\n"," '[UNK]',\n"," 'i',\n"," 'you',\n"," 'to',\n"," 'the',\n"," 'a',\n"," 'is',\n"," 'tom',\n"," 'that',\n"," 'he',\n"," 'do',\n"," 'of',\n"," 'it',\n"," 'this']"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["Checking the french vocabulary"],"metadata":{"id":"qNaPXMOzgxGx"}},{"cell_type":"code","source":["vocab_out_size = len(fra_vectorization.get_vocabulary())\n","\n","vocab_out_size"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w5erRV3egd8A","outputId":"52d59e04-585b-4b4c-f7da-e6712a8ae0ed"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["15000"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["fra_vectorization.get_vocabulary()[:15]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8NO0RIhhgPH4","outputId":"33ed01ee-3e2c-4488-ceb5-eabb9052fedb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['',\n"," '[UNK]',\n"," '[start]',\n"," '[end]',\n"," 'je',\n"," 'de',\n"," 'pas',\n"," 'que',\n"," 'ne',\n"," 'à',\n"," 'la',\n"," 'le',\n"," 'vous',\n"," 'il',\n"," 'tom']"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["Next, formatting our datasets.\n","\n","At each training step, the model will seek to predict target words N+1 (and beyond) using the source sentence and the target words 0 to N.\n","\n","As such, the training dataset will yield a tuple (inputs, targets), where:\n","\n","- inputs is a dictionary with the keys encoder_inputs and decoder_inputs.\n","- encoder_inputs is the vectorized source sentence and decoder_inputs is the target sentence \"so far\", that is to say, the words 0 to N used to predict word N+1 (and beyond) in the target sentence.\n","- target is the target sentence offset by one step: it provides the next words in the target sentence — what the model will try to predict."],"metadata":{"id":"sqgNAvzzhNHZ"}},{"cell_type":"code","source":["def format_dataset(eng, fra):\n","    eng = eng_vectorization(eng)\n","    fra = fra_vectorization(fra)\n","\n","    # ({english tokens, french tokens}, french tokens shifted by 1)\n","    return (\n","        {\n","            \"encoder_inputs\": eng,\n","            \"decoder_inputs\": fra[:, :-1],\n","        },\n","        fra[:, 1:],\n","    )\n","\n","\n","def make_dataset(pairs):\n","    eng_texts, fra_texts = zip(*pairs)\n","\n","    eng_texts = list(eng_texts)\n","    fra_texts = list(fra_texts)\n","\n","    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, fra_texts))\n","\n","    dataset = dataset.batch(batch_size)\n","\n","    dataset = dataset.map(format_dataset)\n","\n","    return dataset.shuffle(2048).prefetch(16).cache()\n","\n","\n","train_ds = make_dataset(train_pairs)\n","val_ds = make_dataset(val_pairs)"],"metadata":{"id":"OI7Xduowtjbt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" inputs, targets = next(iter(train_ds))\n","\n"," print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n"," print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n"," print(f\"targets.shape: {targets.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sGDoTJIctjZb","outputId":"0dbfe048-c040-4bf1-8529-e53a3f289a0b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["inputs[\"encoder_inputs\"].shape: (64, 20)\n","inputs[\"decoder_inputs\"].shape: (64, 20)\n","targets.shape: (64, 20)\n"]}]},{"cell_type":"markdown","source":["Checking the inputs and targets..Note that target is offset by one step from decoder_inputs"],"metadata":{"id":"BTr-iCytktBt"}},{"cell_type":"code","source":["inputs[\"encoder_inputs\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"84UHxVS2tjVp","outputId":"d3b4b5f2-d42c-4546-f9e7-0d75cb1b5db1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(64, 20), dtype=int64, numpy=\n","array([[  10, 3326,   16, ...,    0,    0,    0],\n","       [  41,   30,  716, ...,    0,    0,    0],\n","       [   2,  282,    2, ...,    0,    0,    0],\n","       ...,\n","       [   7,    9,    6, ...,    0,    0,    0],\n","       [  47,    6,  767, ...,    0,    0,    0],\n","       [   3,  129,    6, ...,    0,    0,    0]])>"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["inputs[\"decoder_inputs\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UdDZdNVliZEl","outputId":"729f5531-1b3f-4ff5-e14e-b51c6bd8a4da"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(64, 20), dtype=int64, numpy=\n","array([[  2,  13,  37, ...,   0,   0,   0],\n","       [  2,  12, 259, ...,   0,   0,   0],\n","       [  2, 133,   8, ...,   0,   0,   0],\n","       ...,\n","       [  2,  89,  16, ...,   0,   0,   0],\n","       [  2,  29,  16, ...,   0,   0,   0],\n","       [  2,  19, 117, ...,   0,   0,   0]])>"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["targets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C6ctEtRKkno9","outputId":"f02b639b-140e-4b83-f237-b232c32a903b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(64, 20), dtype=int64, numpy=\n","array([[  13,   37,    1, ...,    0,    0,    0],\n","       [  12,  259,    6, ...,    0,    0,    0],\n","       [ 133,    8,    6, ...,    0,    0,    0],\n","       ...,\n","       [  89,   16, 1967, ...,    0,    0,    0],\n","       [  29,   16, 9255, ...,    0,    0,    0],\n","       [  19,  117,   38, ...,    0,    0,    0]])>"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["Our sequence-to-sequence Transformer consists of a TransformerEncoder and a TransformerDecoder chained together. To make the model aware of word order, we also use a PositionalEmbedding layer.\n","\n","The source sequence will be pass to the TransformerEncoder, which will produce a new representation of it. This new representation will then be passed to the TransformerDecoder, together with the target sequence so far (target words 0 to N). The TransformerDecoder will then seek to predict the next words in the target sequence (N+1 and beyond).\n","\n","A key detail that makes this possible is causal masking (see method get_causal_attention_mask() on the TransformerDecoder). The TransformerDecoder sees the entire sequences at once, and thus we must make sure that it only uses information from target tokens 0 to N when predicting token N+1 (otherwise, it could use information from the future, which would result in a model that cannot be used at inference time).\n","\n"],"metadata":{"id":"0Ic_W7u2lCxC"}},{"cell_type":"code","source":["class TransformerEncoder(layers.Layer):\n","    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.num_heads = num_heads\n","\n","        self.attention = layers.MultiHeadAttention(\n","            num_heads = num_heads, key_dim = embed_dim\n","        )\n","\n","        self.dense_proj = keras.Sequential(\n","            [\n","                layers.Dense(dense_dim, activation = \"relu\"),\n","                layers.Dense(embed_dim),\n","            ]\n","        )\n","\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","\n","        self.supports_masking = True\n","\n","    def call(self, inputs, mask = None):\n","        attention_output = self.attention(query = inputs, value = inputs, key = inputs)\n","\n","        proj_input = self.layernorm_1(inputs + attention_output)\n","\n","        proj_output = self.dense_proj(proj_input)\n","\n","        return self.layernorm_2(proj_input + proj_output)\n","\n","    def get_config(self):\n","        config = super().get_config()\n","\n","        config.update(\n","            {\n","                \"embed_dim\": self.embed_dim,\n","                \"dense_dim\": self.dense_dim,\n","                \"num_heads\": self.num_heads,\n","            }\n","        )\n","\n","        return config"],"metadata":{"id":"_YiWq-b0lb4d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class PositionalEmbedding(layers.Layer):\n","    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        self.token_embeddings = layers.Embedding(\n","            input_dim = vocab_size, output_dim = embed_dim\n","        )\n","\n","        self.position_embeddings = layers.Embedding(\n","            input_dim = sequence_length, output_dim = embed_dim\n","        )\n","\n","        self.sequence_length = sequence_length\n","        self.vocab_size = vocab_size\n","        self.embed_dim = embed_dim\n","\n","    def call(self, inputs):\n","        length = tf.shape(inputs)[-1]\n","        positions = tf.range(start = 0, limit = length, delta = 1)\n","\n","        embedded_tokens = self.token_embeddings(inputs)\n","        embedded_positions = self.position_embeddings(positions)\n","\n","        return embedded_tokens + embedded_positions\n","\n","    def compute_mask(self, inputs, mask = None):\n","        return tf.math.not_equal(inputs, 0)\n","\n","    def get_config(self):\n","        config = super().get_config()\n","\n","        config.update(\n","            {\n","                \"sequence_length\": self.sequence_length,\n","                \"vocab_size\": self.vocab_size,\n","                \"embed_dim\": self.embed_dim,\n","            }\n","        )\n","\n","        return config"],"metadata":{"id":"wyzbTVfNtZHW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Decoder is defined"],"metadata":{"id":"oyzNEX-no2by"}},{"cell_type":"code","source":["class TransformerDecoder(layers.Layer):\n","    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        self.embed_dim = embed_dim\n","        self.latent_dim = latent_dim\n","        self.num_heads = num_heads\n","\n","        self.attention_1 = layers.MultiHeadAttention(\n","            num_heads = num_heads, key_dim = embed_dim\n","        )\n","        self.attention_2 = layers.MultiHeadAttention(\n","            num_heads = num_heads, key_dim = embed_dim\n","        )\n","\n","        self.dense_proj = keras.Sequential(\n","            [\n","                layers.Dense(latent_dim, activation = \"relu\"),\n","                layers.Dense(embed_dim),\n","            ]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.layernorm_3 = layers.LayerNormalization()\n","\n","        self.add = layers.Add()  # instead of `+` to preserve mask\n","\n","        self.supports_masking = True\n","\n","    def call(self, inputs, encoder_outputs, mask = None):\n","        attention_output_1 = self.attention_1(\n","            query = inputs, value = inputs, key = inputs, use_causal_mask = True\n","        )\n","        out_1 = self.layernorm_1(self.add([inputs, attention_output_1]))\n","\n","        attention_output_2 = self.attention_2(\n","            query = out_1,\n","            value = encoder_outputs,\n","            key = encoder_outputs,\n","        )\n","        out_2 = self.layernorm_2(self.add([out_1, attention_output_2]))\n","\n","        proj_output = self.dense_proj(out_2)\n","\n","        return self.layernorm_3(self.add([out_2, proj_output]))\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update(\n","            {\n","                \"embed_dim\": self.embed_dim,\n","                \"latent_dim\": self.latent_dim,\n","                \"num_heads\": self.num_heads,\n","            }\n","        )\n","        return config"],"metadata":{"id":"MlhwuenhtjS5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Assembling the end to end model"],"metadata":{"id":"_hf5R_CwltWe"}},{"cell_type":"code","source":["embed_dim = 256\n","latent_dim = 2048\n","num_heads = 8\n","\n","# Encoder\n","encoder_inputs = keras.Input(shape = (None,), dtype = \"int64\", name = \"encoder_inputs\")\n","\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n","\n","encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n","\n","encoder = keras.Model(encoder_inputs, encoder_outputs)\n","\n","# Decoder\n","decoder_inputs = keras.Input(shape = (None,), dtype = \"int64\", name = \"decoder_inputs\")\n","\n","encoded_seq_inputs = keras.Input(shape = (None, embed_dim), name = \"decoder_state_inputs\")\n","\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n","\n","x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n","\n","x = layers.Dropout(0.5)(x)\n","decoder_outputs = layers.Dense(vocab_size, activation = \"softmax\")(x)\n","\n","decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n","\n","decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n","\n","# Transformer\n","transformer = keras.Model(\n","    [encoder_inputs, decoder_inputs], decoder_outputs, name = \"transformer\"\n",")\n"],"metadata":{"id":"Q1S5eountjQP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We'll use accuracy as a quick way to monitor training progress on the validation data. Note that machine translation typically uses BLEU scores as well as other metrics, rather than accuracy."],"metadata":{"id":"lqzH9mwinpZy"}},{"cell_type":"code","source":["epochs = 30  # This should be at least 30 for convergence\n","\n","transformer.summary()\n","\n","transformer.compile(\n","    \"rmsprop\", loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]\n",")\n","\n","transformer.fit(train_ds, epochs = epochs, validation_data = val_ds)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m2hgPLWrtjN4","outputId":"9b14f0ce-40df-4fb5-b012-fc319ff320dc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"transformer\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," encoder_inputs (InputLayer  [(None, None)]               0         []                            \n"," )                                                                                                \n","                                                                                                  \n"," positional_embedding_1 (Po  (None, None, 256)            3845120   ['encoder_inputs[0][0]']      \n"," sitionalEmbedding)                                                                               \n","                                                                                                  \n"," decoder_inputs (InputLayer  [(None, None)]               0         []                            \n"," )                                                                                                \n","                                                                                                  \n"," transformer_encoder (Trans  (None, None, 256)            3155456   ['positional_embedding_1[0][0]\n"," formerEncoder)                                                     ']                            \n","                                                                                                  \n"," model_1 (Functional)        (None, None, 15000)          1295964   ['decoder_inputs[0][0]',      \n","                                                          0          'transformer_encoder[0][0]'] \n","                                                                                                  \n","==================================================================================================\n","Total params: 19960216 (76.14 MB)\n","Trainable params: 19960216 (76.14 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","__________________________________________________________________________________________________\n","Epoch 1/30\n","1828/1828 [==============================] - 142s 72ms/step - loss: 3.2486 - accuracy: 0.5058 - val_loss: 2.2407 - val_accuracy: 0.6281\n","Epoch 2/30\n","1828/1828 [==============================] - 124s 68ms/step - loss: 2.2340 - accuracy: 0.6364 - val_loss: 1.9451 - val_accuracy: 0.6703\n","Epoch 3/30\n","1828/1828 [==============================] - 124s 68ms/step - loss: 1.9911 - accuracy: 0.6723 - val_loss: 1.8732 - val_accuracy: 0.6847\n","Epoch 4/30\n","1828/1828 [==============================] - 124s 68ms/step - loss: 1.8692 - accuracy: 0.6932 - val_loss: 1.8336 - val_accuracy: 0.6950\n","Epoch 5/30\n","1828/1828 [==============================] - 124s 68ms/step - loss: 1.7977 - accuracy: 0.7064 - val_loss: 1.8181 - val_accuracy: 0.7024\n","Epoch 6/30\n","1828/1828 [==============================] - 124s 68ms/step - loss: 1.7483 - accuracy: 0.7167 - val_loss: 1.8183 - val_accuracy: 0.7032\n","Epoch 7/30\n","1828/1828 [==============================] - 124s 68ms/step - loss: 1.7068 - accuracy: 0.7254 - val_loss: 1.8391 - val_accuracy: 0.7062\n","Epoch 8/30\n","1828/1828 [==============================] - 124s 68ms/step - loss: 1.6729 - accuracy: 0.7318 - val_loss: 1.8630 - val_accuracy: 0.7040\n","Epoch 9/30\n","1828/1828 [==============================] - 132s 72ms/step - loss: 1.6451 - accuracy: 0.7380 - val_loss: 1.8498 - val_accuracy: 0.7091\n","Epoch 10/30\n","1828/1828 [==============================] - 124s 68ms/step - loss: 1.6171 - accuracy: 0.7432 - val_loss: 1.8663 - val_accuracy: 0.7093\n","Epoch 11/30\n","1828/1828 [==============================] - 124s 68ms/step - loss: 1.5914 - accuracy: 0.7482 - val_loss: 1.8815 - val_accuracy: 0.7092\n","Epoch 12/30\n","1828/1828 [==============================] - 124s 68ms/step - loss: 1.5694 - accuracy: 0.7528 - val_loss: 1.8852 - val_accuracy: 0.7111\n","Epoch 13/30\n","1828/1828 [==============================] - 124s 68ms/step - loss: 1.5482 - accuracy: 0.7568 - val_loss: 1.8984 - val_accuracy: 0.7140\n","Epoch 14/30\n","1828/1828 [==============================] - 124s 68ms/step - loss: 1.5281 - accuracy: 0.7608 - val_loss: 1.9269 - val_accuracy: 0.7107\n","Epoch 15/30\n","1828/1828 [==============================] - 124s 68ms/step - loss: 1.5086 - accuracy: 0.7647 - val_loss: 1.9204 - val_accuracy: 0.7133\n","Epoch 16/30\n","1828/1828 [==============================] - 124s 68ms/step - loss: 1.4891 - accuracy: 0.7684 - val_loss: 1.9497 - val_accuracy: 0.7147\n","Epoch 17/30\n","1828/1828 [==============================] - 131s 72ms/step - loss: 1.4710 - accuracy: 0.7716 - val_loss: 1.9555 - val_accuracy: 0.7151\n","Epoch 18/30\n","1828/1828 [==============================] - 124s 68ms/step - loss: 1.4542 - accuracy: 0.7752 - val_loss: 1.9513 - val_accuracy: 0.7172\n","Epoch 19/30\n","1828/1828 [==============================] - 124s 68ms/step - loss: 1.4374 - accuracy: 0.7778 - val_loss: 1.9861 - val_accuracy: 0.7122\n","Epoch 20/30\n","1828/1828 [==============================] - 124s 68ms/step - loss: 1.4219 - accuracy: 0.7808 - val_loss: 1.9859 - val_accuracy: 0.7141\n","Epoch 21/30\n","1828/1828 [==============================] - 125s 68ms/step - loss: 1.4095 - accuracy: 0.7831 - val_loss: 2.0026 - val_accuracy: 0.7161\n","Epoch 22/30\n","1828/1828 [==============================] - 125s 68ms/step - loss: 1.3951 - accuracy: 0.7857 - val_loss: 1.9956 - val_accuracy: 0.7192\n","Epoch 23/30\n","1828/1828 [==============================] - 132s 72ms/step - loss: 1.3813 - accuracy: 0.7880 - val_loss: 2.0265 - val_accuracy: 0.7164\n","Epoch 24/30\n","1828/1828 [==============================] - 124s 68ms/step - loss: 1.3702 - accuracy: 0.7906 - val_loss: 2.0396 - val_accuracy: 0.7164\n","Epoch 25/30\n","1828/1828 [==============================] - 124s 68ms/step - loss: 1.3602 - accuracy: 0.7923 - val_loss: 2.0572 - val_accuracy: 0.7171\n","Epoch 26/30\n","1828/1828 [==============================] - 125s 68ms/step - loss: 1.3478 - accuracy: 0.7942 - val_loss: 2.0737 - val_accuracy: 0.7164\n","Epoch 27/30\n","1828/1828 [==============================] - 124s 68ms/step - loss: 1.3388 - accuracy: 0.7966 - val_loss: 2.0748 - val_accuracy: 0.7173\n","Epoch 28/30\n","1828/1828 [==============================] - 124s 68ms/step - loss: 1.3287 - accuracy: 0.7981 - val_loss: 2.0683 - val_accuracy: 0.7186\n","Epoch 29/30\n","1828/1828 [==============================] - 123s 68ms/step - loss: 1.3212 - accuracy: 0.8000 - val_loss: 2.0906 - val_accuracy: 0.7197\n","Epoch 30/30\n","1828/1828 [==============================] - 124s 68ms/step - loss: 1.3126 - accuracy: 0.8018 - val_loss: 2.1358 - val_accuracy: 0.7177\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7e67a02f7100>"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["fra_vocab = fra_vectorization.get_vocabulary()\n","fra_index_lookup = dict(zip(range(len(fra_vocab)), fra_vocab))\n","max_decoded_sentence_length = 20\n","\n","def decode_sequence(input_sentence):\n","    tokenized_input_sentence = eng_vectorization([input_sentence])\n","    decoded_sentence = \"[start]\"\n","\n","\n","    for i in range(max_decoded_sentence_length):\n","        # Tokenize the sentence generated so far\n","        tokenized_target_sentence = fra_vectorization([decoded_sentence])[:, :-1]\n","\n","        # Predict the next word in the sequence using input sentence and\n","        # tokenized representation of sentence so far\n","        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n","\n","        # Look up the index of the current predicted word\n","        sampled_token_index = np.argmax(predictions[0, i, :])\n","\n","        # Find the corresponding token from the vocabulary\n","        sampled_token = fra_index_lookup[sampled_token_index]\n","\n","        # Add the predicted word to the decoded text so far\n","        decoded_sentence += \" \" + sampled_token\n","\n","        if sampled_token == \"[end]\":\n","            break\n","\n","    return decoded_sentence"],"metadata":{"id":"QKjHL2A1tjLH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_eng_texts = [pair[0] for pair in test_pairs]\n","\n","for _ in range(30):\n","    ip = random.choice(test_eng_texts)\n","    print(\"input :\",ip)\n","    print(\"translation :\", decode_sequence(ip))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m9uNJNdp2Y7e","outputId":"9d985fe8-c954-48d6-99e1-c271e711d215"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["input : We have broken off relations with them.\n","translation : [start] nous avons cassé les [UNK] des mots en panne [end]\n","input : I spilled the milk.\n","translation : [start] jai frappé le lait [end]\n","input : Why exactly did you do that?\n","translation : [start] pourquoi avezvous fait cela exactement [end]\n","input : Almost all of the dogs are alive.\n","translation : [start] presque tous les chiens sont vivant [end]\n","input : He donated countless pieces to the museum.\n","translation : [start] il a fait semblant en [UNK] le musée [end]\n","input : This book is much more interesting than that one.\n","translation : [start] ce livre est bien plus intéressant que celleci [end]\n","input : It would never have occurred to me to say anything like that.\n","translation : [start] Ça ne me ferait jamais là cela lui à dire quelque chose [end]\n","input : They forgot to lock the door.\n","translation : [start] elles ont oublié de fermer la porte à clé [end]\n","input : He was angry that I had insulted him.\n","translation : [start] il était en colère que je lui ai échappé [end]\n","input : I don't know where he lives.\n","translation : [start] je ne sais pas où il vit [end]\n","input : I have to go. It's getting late.\n","translation : [start] je dois aller il me faisait en retard [end]\n","input : I love seeing you so happy.\n","translation : [start] jadore te voir aussi heureux [end]\n","input : I think you sent me the wrong document.\n","translation : [start] je pense que tu mas envoyé le mauvais [UNK] [end]\n","input : Tom speaks to his father in French and his mother in English.\n","translation : [start] tom parle avec son père en français et sa mère langlais [end]\n","input : You need to help me do this.\n","translation : [start] il te faut maider à faire ceci [end]\n","input : I'll leave it up to you.\n","translation : [start] je te laisse la décision [end]\n","input : Do you like French wine?\n","translation : [start] aimezvous le vin français [end]\n","input : I feel giddy.\n","translation : [start] je me sens pris de la tête [end]\n","input : You're not very funny.\n","translation : [start] vous nêtes pas très drôle [end]\n","input : I've finished writing the letter.\n","translation : [start] jai terminé décrire la lettre [end]\n","input : Tom is arguing with his wife.\n","translation : [start] tom est avec toi en train de disputer [end]\n","input : I did try to warn you.\n","translation : [start] je voulais essayer [end]\n","input : I can't believe I did this.\n","translation : [start] je narrive pas à croire que jai fait ceci [end]\n","input : It's your civic duty to vote.\n","translation : [start] tout ton devoir est de voté pour courir [end]\n","input : He earned as much money as possible.\n","translation : [start] il a gagné autant dargent que possible [end]\n","input : May I do it right now?\n","translation : [start] puisje le faire sans [UNK] [end]\n","input : He has a racket.\n","translation : [start] il a une vaisselle [end]\n","input : I've got a pair of sunglasses.\n","translation : [start] jai une paire de lunettes de soleil [end]\n","input : This box contains five apples.\n","translation : [start] cette boîte fonctionne à cinq [UNK] [end]\n","input : I am worried about my mother's health.\n","translation : [start] je me fais du souci pour la santé de ma mère [end]\n"]}]},{"cell_type":"code","source":["for test_pair in test_pairs[:30]:\n","    input_sentence = test_pair[0]\n","    reference_sentence = test_pair[1]\n","\n","    translated_sentence = decode_sequence(input_sentence)\n","    translated_sentence = (\n","        translated_sentence.replace(\"[PAD]\", \"\")\n","        .replace(\"[start]\", \"\")\n","        .replace(\"[end]\", \"\")\n","        .strip()\n","    )\n","\n","    reference_sentence = (\n","        reference_sentence.replace(\"[PAD]\", \"\")\n","        .replace(\"[start]\", \"\")\n","        .replace(\"[end]\", \"\")\n","        .strip()\n","    )\n","\n","    print(\"English: \", input_sentence)\n","    print(\"Translation: \", reference_sentence)\n","    print(\"Prediction: \", translated_sentence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lo7zKDRAG0YC","outputId":"d074421a-7115-45d0-a7ae-b5e3a7a38312"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["English:  We went to see our neighbors.\n","Translation:  Nous sommes allés voir nos voisins.\n","Prediction:  nous sommes allés voir nos voisins\n","English:  I'm not busy anymore.\n","Translation:  Je ne suis plus occupé.\n","Prediction:  je ne suis plus occupé\n","English:  We should save money for a rainy day.\n","Translation:  Nous devrions mettre le l'argent de côté.\n","Prediction:  nous devrions sauver largent pour un jour de pluie\n","English:  Tom is looking for that.\n","Translation:  Tom le cherche.\n","Prediction:  tom cherche cela\n","English:  I wonder who took it.\n","Translation:  Je me demande qui l'a prise.\n","Prediction:  je me demande qui la pris\n","English:  This safe is for keeping valuables.\n","Translation:  Ce coffre est destiné à conserver les objets de valeur.\n","Prediction:  cest en sécurité pour garder les valeur de valeur\n","English:  Is there much demand for these goods?\n","Translation:  Y a-t-il beaucoup de demande pour ces marchandises ?\n","Prediction:  y atil vraiment [UNK] des marchandises pour ces [UNK]\n","English:  I fell asleep listening to music.\n","Translation:  Je me suis endormi en écoutant de la musique.\n","Prediction:  je suis tombé en train découter de la musique\n","English:  Do you know why?\n","Translation:  Est-ce que vous savez pourquoi ?\n","Prediction:  saistu pourquoi\n","English:  My German vocabulary list is up to two thousand words now.\n","Translation:  Ma liste de vocabulaire allemand comporte désormais deux mille mots.\n","Prediction:  mon [UNK] du bien [UNK] est de mots en deux mille mots\n","English:  Can I write it like that?\n","Translation:  On peut écrire comme ça ?\n","Prediction:  puisje écrire comme ça\n","English:  I'm not going to lie to you.\n","Translation:  Je ne vais pas te mentir.\n","Prediction:  je ne vais pas te dire\n","English:  The game will be held rain or shine.\n","Translation:  La partie aura lieu, qu'il pleuve ou qu'il fasse beau.\n","Prediction:  la partie sera [UNK] dici ou [UNK]\n","English:  There are students in the library.\n","Translation:  Il y a des étudiants dans la bibliothèque.\n","Prediction:  il y a des étudiants à la bibliothèque\n","English:  He has a few friends in this town.\n","Translation:  Il a quelques amis dans cette ville.\n","Prediction:  il a quelques amis de cette ville\n","English:  What time is dinner?\n","Translation:  À quelle heure est le déjeuner ?\n","Prediction:  À quelle heure est le dîner\n","English:  She looked at him angrily.\n","Translation:  Elle le regarda avec colère.\n","Prediction:  elle le fit en colère\n","English:  Roll down your window.\n","Translation:  Abaissez votre fenêtre !\n","Prediction:  y astu [UNK]\n","English:  I'm not married yet.\n","Translation:  Je ne suis pas encore marié.\n","Prediction:  je ne suis pas encore mariée\n","English:  The phone is out of order.\n","Translation:  Le téléphone ne fonctionne pas.\n","Prediction:  le téléphone est hors de courir\n","English:  You didn't need to tell me that.\n","Translation:  Tu n'avais pas besoin de me dire ça.\n","Prediction:  tu nas pas besoin de me dire ça\n","English:  You don't have to worry about publicity.\n","Translation:  Il ne faut pas vous en faire pour la publicité.\n","Prediction:  il ne faut pas te faire pour la de la peine de faire de souci pour toi\n","English:  You shouldn't eat anything cold.\n","Translation:  Vous ne devriez rien manger de froid.\n","Prediction:  tu ne devrais rien manger de froid\n","English:  We saw it.\n","Translation:  Nous l'avons vu.\n","Prediction:  nous lavons vue\n","English:  Leave us alone.\n","Translation:  Fiche-nous la paix !\n","Prediction:  laisse tranquille\n","English:  I know something you don't.\n","Translation:  Je sais quelque chose que tu ne sais pas.\n","Prediction:  je sais quelque chose que tu nas pas\n","English:  Tom is at church right now.\n","Translation:  Tom est à l'église en ce moment.\n","Prediction:  tom est à léglise actuellement à léglise\n","English:  This is my assistant.\n","Translation:  C'est mon assistante.\n","Prediction:  ceci est mon [UNK]\n","English:  Here's the key to your room.\n","Translation:  Voici la clé de ta chambre.\n","Prediction:  voici la clé de votre chambre\n","English:  She decided to go.\n","Translation:  Elle a décidé de partir.\n","Prediction:  elle a décidé dy aller\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Atcres5mtjBN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"qHYhaDAEti_E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Y2i6I1EYti8R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"RK1d50COti5d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"EewWBx6Oti2y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"b8Du8OCHti0K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"TBwb2JAwtixo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"DGVsRrodtiu7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"nG7z7EEmtisI"},"execution_count":null,"outputs":[]}]}